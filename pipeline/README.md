vod_pipeline/
â”‚
â”œâ”€â”€ vod_pipeline/ # Main package
â”‚ â”œâ”€â”€ **init**.py
â”‚ â”œâ”€â”€ config.py # Settings loader (envvars, .env, etc.)
â”‚ â”œâ”€â”€ celery_app.py # Celery app config
â”‚ â”œâ”€â”€ tasks/ # Celery task modules
â”‚ â”‚ â”œâ”€â”€ **init**.py
â”‚ â”‚ â”œâ”€â”€ ingest.py # Ingest raw files (pull/push/transcode)
â”‚ â”‚ â”œâ”€â”€ metadata.py # Extract/transform metadata
â”‚ â”‚ â”œâ”€â”€ publish.py # Publish to CMS (via API)
â”‚ â”‚ â”œâ”€â”€ package.py # Create HLS/DASH adaptive packages
â”‚ â”‚ â””â”€â”€ upload.py # Upload to CDN/streaming host
â”‚ â”œâ”€â”€ services/ # Integration clients
â”‚ â”‚ â”œâ”€â”€ **init**.py
â”‚ â”‚ â”œâ”€â”€ cms_client.py # REST API client for CMS
â”‚ â”‚ â”œâ”€â”€ transcoder.py # Local or remote transcoder wrapper (FFmpeg or API)
â”‚ â”‚ â”œâ”€â”€ storage.py # S3, GCS, FTP clients, etc.
â”‚ â”‚ â””â”€â”€ notifier.py # Slack/email/HTTP callbacks, etc.
â”‚ â”œâ”€â”€ workflows/ # Task chaining logic
â”‚ â”‚ â”œâ”€â”€ **init**.py
â”‚ â”‚ â””â”€â”€ vod_ingest_flow.py # Orchestration of the entire VOD pipeline
â”‚ â””â”€â”€ utils/ # Common helpers, e.g. path utils, logging, etc.
â”‚ â”œâ”€â”€ **init**.py
â”‚ â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ tests/ # Unit and integration tests
â”‚ â”œâ”€â”€ conftest.py
â”‚ â”œâ”€â”€ tasks/
â”‚ â”œâ”€â”€ services/
â”‚ â””â”€â”€ workflows/
â”‚
â”œâ”€â”€ docker/ # Optional Docker setup
â”‚ â”œâ”€â”€ Dockerfile
â”‚ â””â”€â”€ celery_worker.sh
â”‚
â”œâ”€â”€ .env # Environment variables (never commit)
â”œâ”€â”€ requirements.txt # Prod deps
â”œâ”€â”€ requirements-dev.txt # Dev/test deps
â”œâ”€â”€ celeryconfig.py # Optional: Celery config if not using env/config.py
â”œâ”€â”€ manage.py # Entrypoint for CLI / management
â”œâ”€â”€ README.md
â””â”€â”€ pyproject.toml / setup.py # Packaging metadata

celery -A vod_pipeline.celery_app worker --loglevel=info 3. Transcoding & Packaging

Modularize your FFmpeg workflows under services/transcoder.py. Abstract whether you're using local FFmpeg or something like AWS MediaConvert.

Same for packagingâ€”HLS/DASH logic should be reusable.

ðŸ”§ Technologies Suggested

    Celery (task queues)

    Redis or RabbitMQ (broker)

    FFmpeg, Shaka Packager, or AWS MediaConvert (transcoding/packaging)

    CMS API (REST/GraphQL)

    S3, GCS, or FTP (storage backend)

    Pydantic / Marshmallow (data validation)

    Typer / Click (CLI entrypoint via manage.py)

    pytest (tests)

ðŸ§  Tips

    Run the ingest flow asynchronously via Celery, but also support replays/retries.

    Use Chord if you need to aggregate results or kick off downstream analysis.

    Separate infra config (env vars) from code logic; use dotenv or pydantic-settings.

Usage

# Run full ingest flow

python manage.py run-workflow "s3://vod-bucket/video123.mov"

# Start worker

python manage.py worker

# Start beat (if using periodic tasks)

python manage.py beat

# Check worker status

python manage.py status

# Retry a task (placeholder logic)

python manage.py replay <task_id>

âœ… Requirements

Add to requirements.txt or requirements-dev.txt:

typer[all]

ðŸ”„ Future Extensions

    schedule-workflow â€“ queue up tasks for later using eta or beat.

    inspect-queue â€“ show pending tasks.

    clear-stuck-tasks â€“ cleanup helper.

    generate-report â€“ export VOD job logs/metadata.

Let me know if you want a CLI command to manually run parts of the pipeline (e.g., just package, upload, etc.).

Example Usage

# Run full chain

python manage.py run-workflow "s3://bucket/asset.mov"

# Run individual tasks

python manage.py task ingest "s3://bucket/asset.mov"
python manage.py task metadata "/tmp/asset.mp4"
python manage.py task package "/tmp/asset.mp4"
python manage.py task upload "/tmp/output/hls/"
python manage.py task publish "/tmp/metadata.json"

# Start worker

python manage.py worker

# Check status

python manage.py status

ðŸ”§ Notes

    All task calls use .delay() to run via Celery.

    The actual paths (media_path, package_path, etc.) depend on your internal pipeline flow.

    These can be extended to include optional parameters like output_dir, profile, force=True, etc.

If you want CLI flags to run tasks synchronously for local debugging (i.e., without Celery), I can add a --sync flag to each. Want that next?
